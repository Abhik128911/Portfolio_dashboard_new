[
  {
  "assignment": "Experiment 1",
  "title": "Web Scraping & API Handling using Python",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To collect data via web-scraping using BeautifulSoup library.</li><li>To interact with APIs (like GitHub) using the Requests library.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre>from bs4 import BeautifulSoup\nimport requests\nimport json\n\n# ==========================================\n# PART 1: Web Scraping using BeautifulSoup\n# ==========================================\nprint('--- PART 1: Web Scraping Output ---')\n\n# Sample HTML document to parse\nhtml_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n<body>\n    <p class =\"title\"><b>The Dormouse's story</b></p>\n    <p class =\"story\">One Upon a time there were three litle sisters; and their names were \n        <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>, \n        <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n        <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n        and they lived at the bottom of the well.\n    </p>\n    <p calss=\"story\">...</p>\n</body>\n</html>\"\"\"\n\n# Parsing the HTML content\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Displaying the formatted HTML structure\nprint(soup.prettify())\n\n# Accessing specific tags from the HTML\nprint('\\nPage Title:', soup.title.string)\nprint('Body Content:', soup.body.text.strip())\n\n# ==========================================\n# PART 2: API Handling using Requests\n# ==========================================\nprint('\\n--- PART 2: API Handling Output ---')\n\n# Making a GET request to the GitHub API\nresponse = requests.get('https://api.github.com')\n\n# Checking the status code (200 means success)\nprint('Status Code:', response.status_code)\n\n# Printing the keys of the JSON response to verify data\nprint('Response JSON Keys:', response.json().keys())</pre>"
    }
  ]
},
  {
  "assignment": "Experiment 2",
  "title": "Handling HTTP Requests with Python",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To perform GET requests using the Requests library.</li><li>To handle HTTP responses, check status codes, and view content.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre>import requests\n\n# ==========================================\n# Handling HTTP Requests\n# ==========================================\n\n# Target URL\nurl = 'https://abhikdas.me'\n\ntry:\n    # Making a GET request to the website\n    print(f'Sending GET request to {url}...')\n    r = requests.get(url)\n\n    # Check the status code of the response\n    # 200 indicates a successful request\n    print('Response Object:', r)\n    print('Status Code:', r.status_code)\n\n    # Accessing and printing the raw HTML content\n    # (Printing first 500 characters to keep output clean)\n    print('\\nResponse Content (First 500 chars):')\n    print(r.text[:500])\n\nexcept requests.exceptions.RequestException as e:\n    # Handling errors (e.g., connection issues)\n    print(f'An error occurred: {e}')</pre>"
    }
  ]
},
  {
  "assignment": "Experiment 3",
  "title": "Data Cleaning: Exploration, Imputation & Visualization",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To inspect dataset for missing values, outliers, and data types.</li><li>To perform data imputation using Mean, Median, Mode, and Interpolation.</li><li>To visualize data distributions using Histograms and Bar Charts.</li><li>To analyze the Titanic dataset and handle missing values in 'Age', 'Cabin', and 'Embarked' columns.</li></ul>"
    },
    {
      "question": "Python Code - Part 1: Sample Data Cleaning",
      "answer": "<pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ==========================================\n# PART 1: Data Cleaning on Sample Data\n# ==========================================\n\n# Step 1: Create a sample dataset with missing values\ndata = {\n    'Name': ['John', 'Mary', 'David', 'Emily', 'Michael'],\n    'Age': [25, 31, np.nan, 42, 28],\n    'City': ['New York', 'Los Angeles', 'Chicago', np.nan, 'Houston']\n}\n\ndf = pd.DataFrame(data)\nprint('Original DataFrame:')\nprint(df)\n\n# Step 2: Data Exploration\nprint('\\nMissing Values:')\nprint(df.isnull().sum())\n\nprint('\\nData Types:')\nprint(df.dtypes)\n\nprint('\\nSummary Statistics:')\nprint(df.describe())\n\n# Step 3: Imputation\n# Impute missing values in 'Age' column with Mean\ndf['Age'].fillna(df['Age'].mean(), inplace=True)\nprint('\\nImputed Dataset (Age Mean):')\nprint(df)\n\n# Reset Data for Median Imputation Demo\nmf = pd.DataFrame(data)\nmf['Age'].fillna(mf['Age'].median(), inplace=True)\nprint('\\nImputed Dataset (Age Median):')\nprint(mf)\n\n# Impute missing values in 'City' column with Mode\ndf['City'].fillna(df['City'].mode()[0], inplace=True)\nprint('\\nImputed Dataset (City Mode):')\nprint(df)\n\n# Impute missing values using Interpolation\niff = pd.DataFrame(data)\niff['Age'].interpolate(method='linear', limit_direction='forward', inplace=True)\nprint('\\nImputed Dataset (Interpolation):')\nprint(iff)\n\n# Step 4: Visualization\n# Plotting Age Distribution\nplt.figure(figsize=(8, 4))\nplt.hist(df['Age'], bins=10, color='pink', alpha=0.7, edgecolor='red')\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\n\n# Plotting City Distribution\ncity_counts = df['City'].value_counts()\nplt.figure(figsize=(8, 4))\nplt.bar(city_counts.index, city_counts.values, color='skyblue', alpha=0.7, edgecolor='blue')\nplt.title('City Distribution')\nplt.xlabel('City')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()</pre>"
    },
    {
      "question": "Python Code - Part 2: Titanic Dataset Analysis",
      "answer": "<pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# ==========================================\n# PART 2: Cleaning Titanic Dataset\n# Note: Requires 'train.csv' and 'test.csv'\n# ==========================================\n\n# Step 1: Load Datasets\ntry:\n    train = pd.read_csv('train.csv')\n    test = pd.read_csv('test.csv')\n\n    print('Train Dataset Head:')\n    print(train.head())\n\n    # Step 2: Exploration\n    print('\\nTrain Data Info:')\n    print(train.info())\n    \n    print('\\nMissing Values (Train):')\n    print(train.isnull().sum())\n    print('\\nMissing Values (Test):')\n    print(test.isnull().sum())\n\n    # Step 3: Imputation\n    # Filling missing 'Age' with Mean\n    train['Age'].fillna(train['Age'].mean(), inplace=True)\n    test['Age'].fillna(test['Age'].mean(), inplace=True)\n\n    # Interpolating 'Cabin' column\n    train['Cabin'].interpolate(method='linear', inplace=True)\n    test['Cabin'].interpolate(method='linear', inplace=True)\n\n    # Filling 'Embarked' with Mode\n    train['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)\n    \n    # Filling remaining 'Cabin' NaNs with Mode (if any left after interpolate)\n    train['Cabin'].fillna(train['Cabin'].mode()[0], inplace=True)\n    test['Cabin'].fillna(test['Cabin'].mode()[0], inplace=True)\n\n    print('\\nMissing Values After Cleaning (Train):')\n    print(train.isnull().sum())\n\n    # Step 4: Visualization\n    # Age Distribution (Train)\n    plt.figure(figsize=(8, 4))\n    plt.hist(train['Age'], bins=30, edgecolor='black')\n    plt.title('Age Distribution in Training Set')\n    plt.xlabel('Age')\n    plt.ylabel('Frequency')\n    plt.show()\n\n    # Fare Distribution (Train)\n    plt.figure(figsize=(8, 4))\n    plt.hist(train['Fare'], bins=30, edgecolor='black')\n    plt.title('Fare Distribution')\n    plt.xlabel('Fare')\n    plt.ylabel('Frequency')\n    plt.show()\n\nexcept FileNotFoundError:\n    print(\"Error: 'train.csv' or 'test.csv' not found. Please upload the datasets.\")</pre>"
    }
  ]
},
{
  "assignment": "Experiment 4",
  "title": "Dimensionality Reduction using PCA and t-SNE",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To understand dimensionality reduction techniques.</li><li>To apply Principal Component Analysis (PCA) on high-dimensional data.</li><li>To apply t-Distributed Stochastic Neighbor Embedding (t-SNE) for nonlinear dimensionality reduction.</li><li>To visualize high-dimensional data in 2D space.</li></ul>"
    },
    {
      "question": "Part 4.1: Principal Component Analysis (PCA)",
      "answer": "<pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load Iris dataset\niris = load_iris()\n\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=iris.feature_names\n)\n\ndf['target'] = iris.target\n\nprint(\"Original Dataset:\")\nprint(df.head())\n\n# Standardize the data\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(df.drop('target', axis=1))\n\n# Create PCA object with 2 components\npca = PCA(n_components=2)\n\n# Fit and transform the data\npca_data = pca.fit_transform(features_scaled)\n\n# Create PCA DataFrame\npca_df = pd.DataFrame(\n    data=pca_data,\n    columns=['PC1', 'PC2']\n)\n\npca_df['target'] = df['target']\n\nprint(\"\\nPCA Dataset:\")\nprint(pca_df.head())\n\n# Plot PCA result\nplt.figure(figsize=(8, 6))\n\nfor target in np.unique(pca_df['target']):\n    plt.scatter(\n        pca_df.loc[pca_df['target'] == target, 'PC1'],\n        pca_df.loc[pca_df['target'] == target, 'PC2'],\n        label=iris.target_names[target]\n    )\n\nplt.title(\"PCA Dimensionality Reduction\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.legend()\nplt.show()</pre>"
    },
    {
      "question": "Part 4.2: t-SNE Dimensionality Reduction",
      "answer": "<pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\n# Load Iris dataset\niris = load_iris()\n\ndf = pd.DataFrame(\n    data=iris.data,\n    columns=iris.feature_names\n)\n\ndf['target'] = iris.target\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(df.drop('target', axis=1))\n\n# Create t-SNE object\ntsne = TSNE(\n    n_components=2,\n    random_state=42,\n    perplexity=30\n)\n\n# Fit and transform data\ntsne_data = tsne.fit_transform(features_scaled)\n\n# Create t-SNE DataFrame\ntsne_df = pd.DataFrame(\n    data=tsne_data,\n    columns=['t-SNE1', 't-SNE2']\n)\n\ntsne_df['target'] = df['target']\n\nprint(\"\\nt-SNE Dataset:\")\nprint(tsne_df.head())\n\n# Plot t-SNE result\nplt.figure(figsize=(8, 6))\n\nfor target in np.unique(tsne_df['target']):\n    plt.scatter(\n        tsne_df.loc[tsne_df['target'] == target, 't-SNE1'],\n        tsne_df.loc[tsne_df['target'] == target, 't-SNE2'],\n        label=iris.target_names[target]\n    )\n\nplt.title(\"t-SNE Dimensionality Reduction\")\nplt.xlabel(\"t-SNE1\")\nplt.ylabel(\"t-SNE2\")\nplt.legend()\nplt.show()</pre>"
    }
  ]
}
,
{
  "assignment": "Experiment 5",
  "title": "Creating a Simple Dataset and Visualizing Data using Histogram and Scatter Plot",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To create a simple dataset using NumPy and Pandas.</li><li>To visualize data distribution using a histogram.</li><li>To understand relationships between variables using a scatter plot.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Create a simple dataset\ndata = {\n    'A': np.random.randn(100),\n    'B': np.random.randn(100),\n    'C': np.random.randn(100),\n    'D': np.random.randn(100)\n}\n\ndf = pd.DataFrame(data)\n\n# Create a histogram for column 'A'\nplt.hist(df['A'], bins=20, edgecolor='black')\nplt.title('Histogram of Column A')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()\n\n# Create a scatter plot for columns 'A' and 'B'\nplt.scatter(df['A'], df['B'])\nplt.title('Scatter Plot of Column A and B')\nplt.xlabel('Column A')\nplt.ylabel('Column B')\nplt.show()</pre>"
    }
  ]
},

  {
  "assignment": "Experiment 6",
  "title": "Association Analysis using Apriori and Eclat Algorithms",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To understand association rule mining.</li><li>To generate frequent itemsets using the Apriori algorithm.</li><li>To generate frequent itemsets using the Eclat algorithm.</li><li>To extract association rules based on confidence.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre>import pandas as pd\nfrom mlxtend.frequent_patterns import apriori, association_rules, eclat\n\n# Load the dataset (Market Basket Optimization)\ndata = pd.read_csv('Market_Basket_optimization.csv', header=None)\nprint('Original Dataset:')\nprint(data.head())\n\n# Convert the dataset into a binary matrix\n\ndef encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n\nbinary_data = data.applymap(encode_units)\nprint('\\nBinary Dataset:')\nprint(binary_data.head())\n\n# Generate frequent itemsets using Apriori\nfrequent_itemsets_apriori = apriori(binary_data, min_support=0.05, use_colnames=True)\nprint('\\nFrequent Itemsets (Apriori):')\nprint(frequent_itemsets_apriori.head())\n\n# Generate association rules using Apriori\nrules_apriori = association_rules(frequent_itemsets_apriori, metric='confidence', min_threshold=0.5)\nprint('\\nAssociation Rules (Apriori):')\nprint(rules_apriori.head())\n\n# Generate frequent itemsets using Eclat\nfrequent_itemsets_eclat = eclat(binary_data, min_support=0.05, use_colnames=True)\nprint('\\nFrequent Itemsets (Eclat):')\nprint(frequent_itemsets_eclat.head())\n\n# Generate association rules using Eclat\nrules_eclat = association_rules(frequent_itemsets_eclat, metric='confidence', min_threshold=0.5)\nprint('\\nAssociation Rules (Eclat):')\nprint(rules_eclat.head())</pre>"
    }
  ]
}
,
  {
    "assignment": "Experiment 7",
    "title": "Association Rule Mining: FP-Growth Algorithm",
    "questions": [
      {
        "question": "Objectives",
        "answer": "<ul><li>To implement the FP-Growth algorithm.</li><li>To calculate the accuracy of generated association rules.</li></ul>"
      },
      {
        "question": "Python Code",
        "answer": "<pre>import pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import fpgrowth, association_rules\n\n# Load and Preprocess Data (Assuming df_encoded is ready from Exp 6 logic)\n# For demo, we reuse the encoded dataframe logic\n\n# ---- FP-Growth Algorithm ----\nfrequent_itemsets_fpgrowth = fpgrowth(df_encoded, min_support=0.4, use_colnames=True)\n\n# Generate Rules\nrules_fpgrowth = association_rules(frequent_itemsets_fpgrowth, metric=\"confidence\", min_threshold=0.5)\n\n# Function to Calculate Accuracy\nN = len(df_encoded)\ndef calc_accuracy(row):\n    support_AB = row['support'] * N\n    support_A = df_encoded[list(row['antecedents'])].all(axis=1).sum()\n    return (support_AB + (N - support_A)) / N\n\nrules_fpgrowth[\"accuracy\"] = rules_fpgrowth.apply(calc_accuracy, axis=1)\n\nprint(\"\\n---- FP-Growth Rules with Accuracy ----\")\nprint(rules_fpgrowth[['antecedents','consequents','support','confidence','lift','accuracy']])</pre>"
      }
    ]
  },
 {
    "assignment": "Experiment 8.1",
    "title": "Comparison of Apriori and FP-Growth",
    "questions": [
      {
        "question": "Objectives",
        "answer": "<ul><li>To compare evaluation metrics (Support, Confidence, Lift, Leverage, Conviction) between Apriori and FP-Growth.</li></ul>"
      },
      {
        "question": "Python Code",
        "answer": "<pre># Assuming rules_apriori and rules_fpgrowth are generated\n\nmetrics_cols = ['antecedents', 'consequents', 'support', 'confidence', 'lift', 'leverage', 'conviction']\n\nprint(\"---- Apriori Rules Metrics ----\")\nprint(rules_apriori[metrics_cols])\n\nprint(\"\\n---- FP-Growth Rules Metrics ----\")\nprint(rules_fpgrowth[metrics_cols])</pre>"
      }
    ]
  },
  {
  "assignment": "Experiment 8.2",
  "title": "Evaluation Metrics of Association Analysis using Apriori and FP-Growth Algorithms",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To evaluate association rule mining using multiple metrics.</li><li>To compare Apriori and FP-Growth algorithms.</li><li>To analyze support, confidence, and lift values.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre>import pandas as pd\nfrom mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n\n# Load dataset\ndata = pd.read_csv('Market_Basket_optimization.csv', header=None)\n\n# Convert dataset to binary format\ndef encode_units(x):\n    return 1 if x >= 1 else 0\n\nbinary_data = data.applymap(encode_units)\n\n# Apriori Algorithm\nfrequent_itemsets_apriori = apriori(binary_data, min_support=0.05, use_colnames=True)\nrules_apriori = association_rules(frequent_itemsets_apriori, metric='confidence', min_threshold=0.5)\n\n# FP-Growth Algorithm\nfrequent_itemsets_fp = fpgrowth(binary_data, min_support=0.05, use_colnames=True)\nrules_fp = association_rules(frequent_itemsets_fp, metric='confidence', min_threshold=0.5)\n\nprint('--- Apriori Rules ---')\nprint(rules_apriori[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())\n\nprint('\\n--- FP-Growth Rules ---')\nprint(rules_fp[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())</pre>"
    }
  ]
}
,
{
  "assignment": "Experiment 9.1",
  "title": "Data Preprocessing using Missing Value Handling, Encoding, Splitting and Feature Scaling",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To preprocess raw dataset for machine learning.</li><li>To handle missing values using mean imputation.</li><li>To encode categorical variables.</li><li>To split the dataset into training and testing sets.</li><li>To apply feature scaling for better model performance.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre># Import required libraries\nimport numpy as np\nimport pandas as pd\n\n# Import the dataset\ndataset = pd.read_csv(\"dataset.csv\")\n\n# Separate independent and dependent variables\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# -------------------------------\n# Handling Missing Data\n# -------------------------------\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nX[:, 1:3] = imputer.fit_transform(X[:, 1:3])\n\n# -------------------------------\n# Encoding Categorical Data\n# -------------------------------\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n# Encode independent variable\nct = ColumnTransformer(\n    transformers=[('encoder', OneHotEncoder(), [0])],\n    remainder='passthrough'\n)\nX = np.array(ct.fit_transform(X))\n\n# Encode dependent variable\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n# -------------------------------\n# Splitting Dataset\n# -------------------------------\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n# -------------------------------\n# Feature Scaling\n# -------------------------------\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nprint(\"Data Preprocessing Completed Successfully!\")</pre>"
    }
  ]
}
,
{
  "assignment": "Experiment 9.2",
  "title": "Data Preprocessing for Recommendation System",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To preprocess raw data for building a recommendation system.</li><li>To handle missing values and normalize the dataset.</li><li>To prepare user-item interaction data.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Sample user-item rating dataset\ndata = {\n    'UserID': [1, 2, 3, 4, 5],\n    'ItemA': [5, 4, None, 2, 1],\n    'ItemB': [3, None, 4, 5, 2],\n    'ItemC': [1, 2, 3, None, 5]\n}\n\ndf = pd.DataFrame(data)\nprint('Original Data:')\nprint(df)\n\n# Fill missing values with column mean\ndf_filled = df.fillna(df.mean())\nprint('\\nData after handling missing values:')\nprint(df_filled)\n\n# Normalize the data\nscaler = MinMaxScaler()\nscaled_values = scaler.fit_transform(df_filled.iloc[:, 1:])\n\ndf_scaled = pd.DataFrame(scaled_values, columns=['ItemA', 'ItemB', 'ItemC'])\ndf_scaled.insert(0, 'UserID', df_filled['UserID'])\n\nprint('\\nNormalized Data:')\nprint(df_scaled)</pre>"
    }
  ]
}
,

{
  "assignment": "Experiment 10",
  "title": "Building, Training and Evaluating an Artificial Neural Network (ANN)",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To build an Artificial Neural Network using Keras.</li><li>To compile the ANN with appropriate optimizer and loss function.</li><li>To train the ANN using training data.</li><li>To evaluate the ANN model using confusion matrix and accuracy.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre># Import required libraries\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# -------------------------------\n# Initializing ANN\n# -------------------------------\nann = Sequential()\n\n# Input Layer & First Hidden Layer\nann.add(Dense(units=6, activation='relu', input_dim=X_train.shape[1]))\n\n# Second Hidden Layer\nann.add(Dense(units=6, activation='relu'))\n\n# Output Layer\nann.add(Dense(units=1, activation='sigmoid'))\n\n# -------------------------------\n# Compiling ANN\n# -------------------------------\nann.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# -------------------------------\n# Training ANN\n# -------------------------------\nann.fit(X_train, y_train, batch_size=32, epochs=100)\n\n# -------------------------------\n# Making Predictions\n# -------------------------------\ny_pred = ann.predict(X_test)\ny_pred = (y_pred > 0.5)\n\n# -------------------------------\n# Model Evaluation\n# -------------------------------\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ncm = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Confusion Matrix:\\n\", cm)\nprint(\"Accuracy:\", accuracy)</pre>"
    }
  ]
}
,
  
  {
  "assignment": "Experiment 11",
  "title": "Recommendation System using SVD Algorithm (Surprise Library)",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To build a recommendation system using collaborative filtering.</li><li>To implement Singular Value Decomposition (SVD) algorithm.</li><li>To split dataset into training and testing sets.</li><li>To evaluate the recommendation system using RMSE and MAE.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre># Install required libraries (run once)\n# pip install scikit-surprise\n# pip install cython\n\n# Import required modules\nfrom surprise import Dataset, Reader, accuracy\nfrom surprise.model_selection import train_test_split\nfrom surprise import SVD\nimport pandas as pd\n\n# Load the dataset\nratings = pd.read_csv(\"ratings.csv\")\n\n# Define rating scale\nreader = Reader(rating_scale=(0.5, 5.0))\n\n# Load data into Surprise format\ndata = Dataset.load_from_df(\n    ratings[['userId', 'movieId', 'rating']],\n    reader\n)\n\n# Split data into training and testing sets\ntrainset, testset = train_test_split(\n    data, test_size=0.2, random_state=42\n)\n\n# Initialize the SVD model\nmodel = SVD()\n\n# Train the model\nmodel.fit(trainset)\n\n# Make predictions\npredictions = model.test(testset)\n\n# Evaluate the model\nprint(\"Evaluation Metrics:\")\n\nrmse = accuracy.rmse(predictions)\nmae = accuracy.mae(predictions)</pre>"
    }
  ]
}

,
{
  "assignment": "Experiment 12",
  "title": "Choropleth Map Visualization using Plotly",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To visualize data using geographical maps.</li><li>To understand choropleth map representation.</li><li>To analyze GDP per capita distribution across countries.</li><li>To use Plotly for interactive data visualization.</li></ul>"
    },
    {
      "question": "Python Code",
      "answer": "<pre># CHOROPLETH MAP VISUALIZATION\n# ================================\n\nimport plotly.express as px\nimport pandas as pd\n\n# Sample dataset\ndata = {\n    \"country\": [\"USA\", \"IND\", \"CHN\", \"DEU\", \"BRA\", \"CAN\", \"RUS\", \"ZAF\", \"JPN\", \"AUS\"],\n    \"gdp_per_capita\": [65000, 2100, 12000, 48000, 9000, 52000, 11500, 6000, 42000, 55000]\n}\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Create choropleth map\nfig = px.choropleth(\n    df,\n    locations=\"country\",\n    locationmode=\"ISO-3\",\n    color=\"gdp_per_capita\",\n    color_continuous_scale=\"viridis\",\n    title=\"GDP per Capita by Country (Sample Data)\",\n    labels={\"gdp_per_capita\": \"GDP per Capita (USD)\"}\n)\n\n# Display map\nfig.show()</pre>"
    }
  ]
}

,
{
  "assignment": "Experiment 13.1",
  "title": "Geospatial Visualization using Static and Interactive Maps",
  "questions": [
    {
      "question": "Objectives",
      "answer": "<ul><li>To visualize geographical data using GeoPandas.</li><li>To create static cartographic maps.</li><li>To develop interactive maps using Folium.</li><li>To explore GeoJSON-based spatial datasets.</li></ul>"
    },
    {
      "question": "Part A: Static Map using GeoPandas",
      "answer": "<pre>import geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Load GeoJSON file\ngeojson_path = \"india_states.geojson\"\ngdf = gpd.read_file(geojson_path)\n\nprint(\"GeoJSON file loaded successfully!\")\nprint(\"Number of features:\", len(gdf))\nprint(\"Columns available:\", list(gdf.columns))\n\n# Display first few rows\ngdf.head()\n\n# Plot static map\nplt.figure(figsize=(10, 10))\ngdf.plot(edgecolor=\"black\", linewidth=0.5, cmap=\"viridis\")\nplt.title(\"Cartographic Visualization of Indian States\", fontsize=16)\nplt.axis(\"off\")\nplt.show()</pre>"
    },
    {
      "question": "Part B: Interactive Map using Folium",
      "answer": "<pre>import folium\nfrom folium.features import GeoJsonTooltip\nfrom IPython.display import IFrame, display\n\n# Center of India\ncenter_lat = 22.9734\ncenter_lon = 78.6569\n\n# Create folium map\nm = folium.Map(\n    location=[center_lat, center_lon],\n    zoom_start=5,\n    tiles=\"CartoDB positron\"\n)\n\n# Possible name field in GeoJSON\nname_field = \"STATE_NAME\"  # change if needed\n\n# Add GeoJSON layer\nfolium.GeoJson(\n    gdf,\n    name=\"Indian States\",\n    tooltip=GeoJsonTooltip(\n        fields=[name_field],\n        aliases=[\"State:\"]\n    ),\n    style_function=lambda x: {\n        \"fillColor\": \"lightgreen\",\n        \"color\": \"black\",\n        \"weight\": 0.5,\n        \"fillOpacity\": 0.6\n    }\n).add_to(m)\n\n# Add layer control\nfolium.LayerControl().add_to(m)\n\n# Save map\noutput_map = \"india_states_map.html\"\nm.save(output_map)\n\nprint(\"Interactive map saved as:\", output_map)\n\n# Display map inside notebook\ndisplay(IFrame(output_map, width=950, height=600))</pre>"
    }
  ]
}
,
  {
    "assignment": "Experiment 13.2",
    "title": "Cartographic Visualization of Indian States using GeoJSON and Folium",
    "questions": [
      {
        "question": "Objectives",
        "answer": "<ul><li>Load and visualize GeoJSON geographical data</li><li>Create static map using GeoPandas and Matplotlib</li><li>Generate interactive map with Folium library</li><li>Add tooltips and styling to geographical features</li></ul>"
      },
      {
        "question": "Python Code",
        "answer": "<pre>import geopandas as gpd\nimport folium\nimport matplotlib.pyplot as plt\nfrom folium.features import GeoJsonTooltip\nfrom IPython.display import IFrame, display\n\n# Step 3: Load the uploaded GeoJSON file\ngeojson_path = \"/content/india_states.geojson\"  # uploaded file path\ngdf = gpd.read_file(geojson_path)\n\nprint(\"‚úÖ GeoJSON file loaded successfully.\")\nprint(\"Number of features:\", len(gdf))\nprint(\"Columns available:\", list(gdf.columns))\n\n# Step 4: Quick preview of the GeoDataFrame\ndisplay(gdf.head())\n\n# Step 5: Simple static plot using GeoPandas + Matplotlib\nplt.figure(figsize=(10, 10))\ngdf.plot(edgecolor=\"black\", linewidth=0.5, cmap=\"viridis\")\nplt.title(\"Cartographic Visualization of Indian States\", fontsize=16)\nplt.axis(\"off\")\nplt.show()\n\n# Step 6: Create an interactive map using Folium\n# Center map over India (approximate lat/lon)\ncenter_lat, center_lon = 22.9734, 78.6569\n\nm = folium.Map(location=[center_lat, center_lon], zoom_start=5, tiles=\"CartoDB positron\")\n\n# Detect a suitable property to use for tooltip (e.g., 'st_nm' or 'STATE_NAME')\npossible_name_fields = ['st_nm', 'STATE_NAME', 'NAME_1', 'NAME']\nname_field = None\nfor field in possible_name_fields:\n    if field in gdf.columns:\n        name_field = field\n        break\n\nif name_field is None:\n    name_field = [col for col in gdf.columns if col != 'geometry'][0]\nprint(f\"Using '{name_field}' as the name field for tooltips.\")\n\n# Add GeoJSON layer with tooltips\nfolium.GeoJson(\n    gdf,\n    name=\"Indian States\",\n    tooltip=GeoJsonTooltip(fields=[name_field], aliases=[\"State: \"]),\n    style_function=lambda x: {\n        'fillColor': 'lightgreen',\n        'color': 'black',\n        'weight': 0.5,\n        'fillOpacity': 0.6\n    }\n).add_to(m)\n\nfolium.LayerControl().add_to(m)\n\n# Step 7: Save and display the interactive map\noutput_map = \"/content/india_states_map.html\"\nm.save(output_map)\nprint(\"üåè Interactive map saved as:\", output_map)\n\ndisplay(IFrame(output_map, width=950, height=600))</pre>"
      }
    ]
  },
  
  {
    "assignment": "Experiment 14",
    "title": "Text Analysis: TF-IDF and Cosine Similarity",
    "questions": [
      {
        "question": "Objectives",
        "answer": "<ul><li>To preprocess text and compute TF-IDF matrix.</li><li>To calculate Cosine Similarity between documents.</li></ul>"
      },
      {
        "question": "Python Code",
        "answer": "<pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\n\ndocuments = [\n    \"Data science is an interdisciplinary field focused on extracting insights from data.\",\n    \"Machine learning is a subset of data science that enables systems to learn automatically.\"\n]\n\n# (Preprocessing steps assumed here)\ncleaned_docs = documents # Placeholder for preprocessed text\n\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(cleaned_docs)\n\n# Convert to DataFrame for visualization\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\nprint(\"\\nTF-IDF Matrix:\\n\")\nprint(tfidf_df.round(3))\n\n# Cosine Similarity\nsimilarity_matrix = cosine_similarity(tfidf_matrix)\nsimilarity_df = pd.DataFrame(similarity_matrix,\n                             index=[f'Doc{i+1}' for i in range(len(documents))],\n                             columns=[f'Doc{i+1}' for i in range(len(documents))])\n\nprint(\"\\nDocument Similarity Matrix:\\n\")\nprint(similarity_df.round(3))</pre>"
      }
    ]
  },
  {
    "assignment": "Experiment 15",
    "title": "Text Visualization: Word Cloud",
    "questions": [
      {
        "question": "Objectives",
        "answer": "<ul><li>To generate and visualize a Word Cloud from text data.</li></ul>"
      },
      {
        "question": "Python Code",
        "answer": "<pre>import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# Sample Text\ntext = \"Data Science Machine Learning Artificial Intelligence...\"\n\n# (Preprocessing function calling assumed)\nprocessed_text = text # Placeholder\n\n# Generate Word Cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(processed_text)\n\n# Render the Word Cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud Visualization')\nplt.show()</pre>"
      }
    ]
  }
]